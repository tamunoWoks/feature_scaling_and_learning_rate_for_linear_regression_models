# Feature scaling and Learning Rate (Multi-variable Linear Regression)
### Goals
In this notebook you will:
- Utilize the multiple variables routines developed in our previous projects
- Run Gradient Descent on a data set with multiple features
- Explore the impact of the learning rate alpha on gradient descent
- Improve performance of gradient descent by feature scaling using z-score normalization
### Tools
We will utilize the functions developed in the last notebook as well as `matplotlib` and `NumPy`.
### Notation
General
|Notation	| Description	| Python (if applicable) |
|---------|-------------|------------------------|
| ğ‘ | scalar, non bold	| |
| ğš |	vector, bold	| |
| ğ€ |	matrix, bold capital	| |
| Regression	|	
| ğ— | training example maxtrix | X_train |
| ğ² |	training example targets	| y_train |
| ğ±<sup>(ğ‘–)</sup>, ğ‘¦<sup>(ğ‘–)</sup> | ğ‘–ğ‘¡â„ Training Example |	X[i], y[i] |
| m	| number of training examples	| m |
| n	| number of features in each example | n |
| ğ° | parameter: weight, |	w |
| ğ‘ | parameter: bias	| b |
| ğ‘“ğ°,ğ‘(ğ±<sup>(ğ‘–)</sup>) | The result of the model evaluation at  ğ±<sup>(ğ‘–)</sup> parameterized by  ğ°,ğ‘ :  ğ‘“ğ°,ğ‘(ğ±<sup>(ğ‘–)</sup>)=ğ°â‹…ğ±<sup>(ğ‘–)</sup>+ğ‘ | f_wb |
| âˆ‚ğ½(ğ°,ğ‘)/âˆ‚ğ‘¤ğ‘— |	the gradient or partial derivative of cost with respect to a parameter  ğ‘¤ğ‘— | dj_dw[j] |
| âˆ‚ğ½(ğ°,ğ‘)/âˆ‚ğ‘ | the gradient or partial derivative of cost with respect to a parameter  ğ‘ | 	dj_db |
